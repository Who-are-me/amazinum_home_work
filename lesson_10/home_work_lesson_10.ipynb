{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c62c9ca-e13d-4ef1-8d41-89f9a935c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i cant see any task except home work from .pdf file - 'Get practice', and i make some evaluation of random models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4464a2d3-965a-4297-bd89-f586ce08cd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.0rc1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ee06c5-715b-411f-9f33-34487d8a8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54e7f071-9187-4c8e-a7f4-201ae1b075c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mnist_model_64.pt'\n",
    "device = 'cuda'\n",
    "batch_size = 64\n",
    "precision_model = 64 #'bf16-mixed'\n",
    "num_workers = 15\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b81016dc-4ecf-4bca-b5ad-97a54a8c2c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(28 * 28, 128)\n",
    "        self.layer_2 = nn.Linear(128, 256)\n",
    "        self.layer_3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        return self.layer_3(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=64, num_workers=8):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(root=\".\", train=True, download=True)\n",
    "        MNIST(root=\".\", train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        mnist_full = MNIST(root=\".\", train=True, transform=self.transform)\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        self.mnist_test = MNIST(root=\".\", train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af156eaa-ac5f-4757-8e7c-52852a3d48a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module pytorch_lightning.trainer.trainer:\n",
      "\n",
      "class Trainer(builtins.object)\n",
      " |  Trainer(*, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator] = 'auto', strategy: Union[str, pytorch_lightning.strategies.strategy.Strategy] = 'auto', devices: Union[list[int], str, int] = 'auto', num_nodes: int = 1, precision: Union[Literal[64, 32, 16], Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'], Literal['64', '32', '16', 'bf16'], NoneType] = None, logger: Union[pytorch_lightning.loggers.logger.Logger, collections.abc.Iterable[pytorch_lightning.loggers.logger.Logger], bool, NoneType] = None, callbacks: Union[list[pytorch_lightning.callbacks.callback.Callback], pytorch_lightning.callbacks.callback.Callback, NoneType] = None, fast_dev_run: Union[int, bool] = False, max_epochs: Optional[int] = None, min_epochs: Optional[int] = None, max_steps: int = -1, min_steps: Optional[int] = None, max_time: Union[str, datetime.timedelta, dict[str, int], NoneType] = None, limit_train_batches: Union[int, float, NoneType] = None, limit_val_batches: Union[int, float, NoneType] = None, limit_test_batches: Union[int, float, NoneType] = None, limit_predict_batches: Union[int, float, NoneType] = None, overfit_batches: Union[int, float] = 0.0, val_check_interval: Union[int, float, NoneType] = None, check_val_every_n_epoch: Optional[int] = 1, num_sanity_val_steps: Optional[int] = None, log_every_n_steps: Optional[int] = None, enable_checkpointing: Optional[bool] = None, enable_progress_bar: Optional[bool] = None, enable_model_summary: Optional[bool] = None, accumulate_grad_batches: int = 1, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None, deterministic: Union[bool, Literal['warn'], NoneType] = None, benchmark: Optional[bool] = None, inference_mode: bool = True, use_distributed_sampler: bool = True, profiler: Union[pytorch_lightning.profilers.profiler.Profiler, str, NoneType] = None, detect_anomaly: bool = False, barebones: bool = False, plugins: Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync, list[Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync]], NoneType] = None, sync_batchnorm: bool = False, reload_dataloaders_every_n_epochs: int = 0, default_root_dir: Union[str, pathlib.Path, NoneType] = None, model_registry: Optional[str] = None) -> None\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator] = 'auto', strategy: Union[str, pytorch_lightning.strategies.strategy.Strategy] = 'auto', devices: Union[list[int], str, int] = 'auto', num_nodes: int = 1, precision: Union[Literal[64, 32, 16], Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'], Literal['64', '32', '16', 'bf16'], NoneType] = None, logger: Union[pytorch_lightning.loggers.logger.Logger, collections.abc.Iterable[pytorch_lightning.loggers.logger.Logger], bool, NoneType] = None, callbacks: Union[list[pytorch_lightning.callbacks.callback.Callback], pytorch_lightning.callbacks.callback.Callback, NoneType] = None, fast_dev_run: Union[int, bool] = False, max_epochs: Optional[int] = None, min_epochs: Optional[int] = None, max_steps: int = -1, min_steps: Optional[int] = None, max_time: Union[str, datetime.timedelta, dict[str, int], NoneType] = None, limit_train_batches: Union[int, float, NoneType] = None, limit_val_batches: Union[int, float, NoneType] = None, limit_test_batches: Union[int, float, NoneType] = None, limit_predict_batches: Union[int, float, NoneType] = None, overfit_batches: Union[int, float] = 0.0, val_check_interval: Union[int, float, NoneType] = None, check_val_every_n_epoch: Optional[int] = 1, num_sanity_val_steps: Optional[int] = None, log_every_n_steps: Optional[int] = None, enable_checkpointing: Optional[bool] = None, enable_progress_bar: Optional[bool] = None, enable_model_summary: Optional[bool] = None, accumulate_grad_batches: int = 1, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None, deterministic: Union[bool, Literal['warn'], NoneType] = None, benchmark: Optional[bool] = None, inference_mode: bool = True, use_distributed_sampler: bool = True, profiler: Union[pytorch_lightning.profilers.profiler.Profiler, str, NoneType] = None, detect_anomaly: bool = False, barebones: bool = False, plugins: Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync, list[Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync]], NoneType] = None, sync_batchnorm: bool = False, reload_dataloaders_every_n_epochs: int = 0, default_root_dir: Union[str, pathlib.Path, NoneType] = None, model_registry: Optional[str] = None) -> None\n",
      " |      Customize every aspect of training via flags.\n",
      " |      \n",
      " |      Args:\n",
      " |          accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"mps\", \"auto\")\n",
      " |              as well as custom accelerator instances.\n",
      " |      \n",
      " |          strategy: Supports different training strategies with aliases as well custom strategies.\n",
      " |              Default: ``\"auto\"``.\n",
      " |      \n",
      " |          devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\n",
      " |              (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\n",
      " |              automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\n",
      " |      \n",
      " |          num_nodes: Number of GPU nodes for distributed training.\n",
      " |              Default: ``1``.\n",
      " |      \n",
      " |          precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n",
      " |              16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n",
      " |              Can be used on CPU, GPU, TPUs, or HPUs.\n",
      " |              Default: ``'32-true'``.\n",
      " |      \n",
      " |          logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
      " |              the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\n",
      " |              ``False`` will disable logging. If multiple loggers are provided, local files\n",
      " |              (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |          callbacks: Add a callback or list of callbacks.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      " |              of train, val and test to find any bugs (ie: a sort of unit test).\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |          max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      " |              If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
      " |              To enable infinite training, set ``max_epochs = -1``.\n",
      " |      \n",
      " |          min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      " |      \n",
      " |          max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
      " |              and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
      " |              ``max_epochs`` to ``-1``.\n",
      " |      \n",
      " |          min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
      " |      \n",
      " |          max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
      " |              The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
      " |              :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
      " |              :class:`datetime.timedelta`.\n",
      " |      \n",
      " |          limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\n",
      " |              Default: ``0.0``.\n",
      " |      \n",
      " |          val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
      " |              after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
      " |              batches. An ``int`` value can only be higher than the number of training batches when\n",
      " |              ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\n",
      " |              across epochs or during iteration-based training.\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          check_val_every_n_epoch: Perform a validation loop after every `N` training epochs. If ``None``,\n",
      " |              validation will be done solely based on the number of training batches, requiring ``val_check_interval``\n",
      " |              to be an integer value.\n",
      " |              Default: ``1``.\n",
      " |      \n",
      " |          num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      " |              Set it to `-1` to run all batches in all validation dataloaders.\n",
      " |              Default: ``2``.\n",
      " |      \n",
      " |          log_every_n_steps: How often to log within steps.\n",
      " |              Default: ``50``.\n",
      " |      \n",
      " |          enable_checkpointing: If ``True``, enable checkpointing.\n",
      " |              It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |          enable_progress_bar: Whether to enable to progress bar by default.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |          enable_model_summary: Whether to enable model summarization by default.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |          accumulate_grad_batches: Accumulates gradients over k batches before stepping the optimizer.\n",
      " |              Default: 1.\n",
      " |      \n",
      " |          gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
      " |              gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
      " |              to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
      " |              be set to ``\"norm\"``.\n",
      " |      \n",
      " |          deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
      " |              Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\n",
      " |              that don't support deterministic mode. If not set, defaults to ``False``. Default: ``None``.\n",
      " |      \n",
      " |          benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\n",
      " |              The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\n",
      " |              (``False`` if not manually set). If :paramref:`~pytorch_lightning.trainer.trainer.Trainer.deterministic`\n",
      " |              is set to ``True``, this will default to ``False``. Override to manually set a different value.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\n",
      " |              evaluation (``validate``/``test``/``predict``).\n",
      " |      \n",
      " |          use_distributed_sampler: Whether to wrap the DataLoader's sampler with\n",
      " |              :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for\n",
      " |              strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and\n",
      " |              ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass\n",
      " |              ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed\n",
      " |              sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,\n",
      " |              we don't do this automatically.\n",
      " |      \n",
      " |          profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |          barebones: Whether to run in \"barebones mode\", where all features that may impact raw speed are\n",
      " |              disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training\n",
      " |              runs. The following features are deactivated:\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_checkpointing`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.logger`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_progress_bar`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.log_every_n_steps`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_model_summary`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.num_sanity_val_steps`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.fast_dev_run`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.detect_anomaly`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.profiler`,\n",
      " |              :meth:`~pytorch_lightning.core.LightningModule.log`,\n",
      " |              :meth:`~pytorch_lightning.core.LightningModule.log_dict`.\n",
      " |          plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |          reload_dataloaders_every_n_epochs: Set to a positive integer to reload dataloaders every n epochs.\n",
      " |              Default: ``0``.\n",
      " |      \n",
      " |          default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      " |              Default: ``os.getcwd()``.\n",
      " |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      " |      \n",
      " |          model_registry: The name of the model being uploaded to Model hub.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``gradient_clip_val`` is not an int or float.\n",
      " |      \n",
      " |          MisconfigurationException:\n",
      " |              If ``gradient_clip_algorithm`` is invalid.\n",
      " |  \n",
      " |  fit(self, model: 'pl.LightningModule', train_dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, val_dataloaders: Optional[Any] = None, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None) -> None\n",
      " |      Runs the full optimization routine.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: Model to fit.\n",
      " |      \n",
      " |          train_dataloaders: An iterable or collection of iterables specifying training samples.\n",
      " |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.train_dataloader` hook.\n",
      " |      \n",
      " |          val_dataloaders: An iterable or collection of iterables specifying validation samples.\n",
      " |      \n",
      " |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.train_dataloader` hook.\n",
      " |      \n",
      " |          ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of two special\n",
      " |              keywords ``\"last\"``, ``\"hpc\"`` and ``\"registry\"``.\n",
      " |              Otherwise, if there is no checkpoint file at the path, an exception is raised.\n",
      " |      \n",
      " |                  - best: the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |                  - last: the last model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |                  - registry: the model will be downloaded from the Lightning Model Registry with following notations:\n",
      " |      \n",
      " |                      - ``'registry'``: uses the latest/default version of default model set\n",
      " |                        with ``Tainer(..., model_registry=\"my-model\")``\n",
      " |                      - ``'registry:model-name'``: uses the latest/default version of this model `model-name`\n",
      " |                      - ``'registry:model-name:version:v2'``: uses the specific version 'v2' of the model `model-name`\n",
      " |                      - ``'registry:version:v2'``: uses the default model set\n",
      " |                        with ``Tainer(..., model_registry=\"my-model\")`` and version 'v2'\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``model`` is not :class:`~pytorch_lightning.core.LightningModule` for torch version less than\n",
      " |              2.0.0 and if ``model`` is not :class:`~pytorch_lightning.core.LightningModule` or\n",
      " |              :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |  \n",
      " |  init_module(self, empty_init: Optional[bool] = None) -> collections.abc.Generator\n",
      " |      Tensors that you instantiate under this context manager will be created on the device right away and have\n",
      " |      the right data type depending on the precision setting in the Trainer.\n",
      " |      \n",
      " |      The parameters and tensors get created on the device and with the right data type right away without wasting\n",
      " |      memory being allocated unnecessarily.\n",
      " |      \n",
      " |      Args:\n",
      " |          empty_init: Whether to initialize the model with empty weights (uninitialized memory).\n",
      " |              If ``None``, the strategy will decide. Some strategies may not support all options.\n",
      " |              Set this to ``True`` if you are loading a checkpoint into a large model.\n",
      " |  \n",
      " |  predict(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None, return_predictions: Optional[bool] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None) -> Union[list[Any], list[list[Any]], NoneType]\n",
      " |      Run inference on your data. This will call the model forward function to compute predictions. Useful to\n",
      " |      perform distributed and batched predictions. Logging is disabled in the predict hooks.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to predict with.\n",
      " |      \n",
      " |          dataloaders: An iterable or collection of iterables specifying predict samples.\n",
      " |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.predict_dataloader` hook.\n",
      " |      \n",
      " |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.predict_dataloader` hook.\n",
      " |      \n",
      " |          return_predictions: Whether to return predictions.\n",
      " |              ``True`` by default except when an accelerator that spawns processes is used (not supported).\n",
      " |      \n",
      " |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\n",
      " |              to predict. If ``None`` and the model instance was passed, use the current weights.\n",
      " |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |              if a checkpoint callback is configured.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      " |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      " |      \n",
      " |          MisconfigurationException:\n",
      " |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      " |      \n",
      " |          RuntimeError:\n",
      " |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      " |      \n",
      " |      See :ref:`Lightning inference section<deploy/production_basic:Predict step with your LightningModule>` for more.\n",
      " |  \n",
      " |  print(self, *args: Any, **kwargs: Any) -> None\n",
      " |      Print something only on the first process. If running on multiple machines, it will print from the first\n",
      " |      process in each machine.\n",
      " |      \n",
      " |      Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\n",
      " |  \n",
      " |  save_checkpoint(self, filepath: Union[str, pathlib.Path], weights_only: bool = False, storage_options: Optional[Any] = None) -> None\n",
      " |      Runs routine to create a checkpoint.\n",
      " |      \n",
      " |      This method needs to be called on all processes in case the selected strategy is handling distributed\n",
      " |      checkpointing.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: Path where checkpoint is saved.\n",
      " |          weights_only: If ``True``, will only save the model weights.\n",
      " |          storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError:\n",
      " |              If the model is not attached to the Trainer before calling this method.\n",
      " |  \n",
      " |  test(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None, verbose: bool = True, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None) -> list[collections.abc.Mapping[str, float]]\n",
      " |      Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your\n",
      " |      test set until you want to.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to test.\n",
      " |      \n",
      " |          dataloaders: An iterable or collection of iterables specifying test samples.\n",
      " |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.test_dataloader` hook.\n",
      " |      \n",
      " |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\n",
      " |              to test. If ``None`` and the model instance was passed, use the current weights.\n",
      " |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |              if a checkpoint callback is configured.\n",
      " |      \n",
      " |          verbose: If True, prints the test results.\n",
      " |      \n",
      " |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.test_dataloader` hook.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks\n",
      " |          like :meth:`~pytorch_lightning.LightningModule.test_step` etc.\n",
      " |          The length of the list corresponds to the number of test dataloaders used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      " |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      " |      \n",
      " |          MisconfigurationException:\n",
      " |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      " |      \n",
      " |          RuntimeError:\n",
      " |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      " |  \n",
      " |  validate(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None, verbose: bool = True, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None) -> list[collections.abc.Mapping[str, float]]\n",
      " |      Perform one evaluation epoch over the validation set.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to validate.\n",
      " |      \n",
      " |          dataloaders: An iterable or collection of iterables specifying validation samples.\n",
      " |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.val_dataloader` hook.\n",
      " |      \n",
      " |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"``, ``\"registry\"`` or path to the checkpoint you wish\n",
      " |              to validate. If ``None`` and the model instance was passed, use the current weights.\n",
      " |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |              if a checkpoint callback is configured.\n",
      " |      \n",
      " |          verbose: If True, prints the validation results.\n",
      " |      \n",
      " |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.val_dataloader` hook.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks\n",
      " |          like :meth:`~pytorch_lightning.LightningModule.validation_step` etc.\n",
      " |          The length of the list corresponds to the number of validation dataloaders used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      " |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      " |      \n",
      " |          MisconfigurationException:\n",
      " |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      " |      \n",
      " |          RuntimeError:\n",
      " |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  accelerator\n",
      " |  \n",
      " |  callback_metrics\n",
      " |      The metrics available to callbacks.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              self.log(\"a_val\", 2.0)\n",
      " |      \n",
      " |      \n",
      " |          callback_metrics = trainer.callback_metrics\n",
      " |          assert callback_metrics[\"a_val\"] == 2.0\n",
      " |  \n",
      " |  checkpoint_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` callback in the\n",
      " |      Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  checkpoint_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` found in\n",
      " |      the Trainer.callbacks list.\n",
      " |  \n",
      " |  current_epoch\n",
      " |      The current epoch, updated after the epoch end hooks are run.\n",
      " |  \n",
      " |  default_root_dir\n",
      " |      The default location to save artifacts of loggers, checkpoints etc.\n",
      " |      \n",
      " |      It is used as a fallback if logger or checkpoint callback do not define specific save paths.\n",
      " |  \n",
      " |  device_ids\n",
      " |      List of device indexes per node.\n",
      " |  \n",
      " |  distributed_sampler_kwargs\n",
      " |  \n",
      " |  early_stopping_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping` callback in the\n",
      " |      Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  early_stopping_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping` found in the\n",
      " |      Trainer.callbacks list.\n",
      " |  \n",
      " |  enable_validation\n",
      " |      Check if we should run validation during training.\n",
      " |  \n",
      " |  estimated_stepping_batches\n",
      " |      The estimated number of batches that will ``optimizer.step()`` during training.\n",
      " |      \n",
      " |      This accounts for gradient accumulation and the current trainer configuration. This might be used when setting\n",
      " |      up your training dataloader, if it hasn't been set up already.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def configure_optimizers(self):\n",
      " |              optimizer = ...\n",
      " |              stepping_batches = self.trainer.estimated_stepping_batches\n",
      " |              scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\n",
      " |              return [optimizer], [scheduler]\n",
      " |      \n",
      " |      Raises:\n",
      " |          MisconfigurationException:\n",
      " |              If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\n",
      " |              at different epochs.\n",
      " |  \n",
      " |  evaluating\n",
      " |  \n",
      " |  global_rank\n",
      " |  \n",
      " |  global_step\n",
      " |      The number of optimizer steps taken (does not reset each epoch).\n",
      " |      \n",
      " |      This includes multiple optimizers (if enabled).\n",
      " |  \n",
      " |  interrupted\n",
      " |  \n",
      " |  is_global_zero\n",
      " |      Whether this process is the global zero in multi-node training.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              if self.trainer.is_global_zero:\n",
      " |                  print(\"in node 0, accelerator 0\")\n",
      " |  \n",
      " |  is_last_batch\n",
      " |      Whether trainer is executing the last batch.\n",
      " |  \n",
      " |  lightning_module\n",
      " |  \n",
      " |  local_rank\n",
      " |  \n",
      " |  log_dir\n",
      " |      The directory for the current experiment. Use this to save images to, etc...\n",
      " |      \n",
      " |      .. note:: You must call this on all processes. Failing to do so will cause your program to stall forever.\n",
      " |      \n",
      " |       .. code-block:: python\n",
      " |      \n",
      " |           def training_step(self, batch, batch_idx):\n",
      " |               img = ...\n",
      " |               save_img(img, self.trainer.log_dir)\n",
      " |  \n",
      " |  logged_metrics\n",
      " |      The metrics sent to the loggers.\n",
      " |      \n",
      " |      This includes metrics logged via :meth:`~pytorch_lightning.core.LightningModule.log` with the\n",
      " |      :paramref:`~pytorch_lightning.core.LightningModule.log.logger` argument set.\n",
      " |  \n",
      " |  lr_scheduler_configs\n",
      " |  \n",
      " |  max_epochs\n",
      " |  \n",
      " |  max_steps\n",
      " |  \n",
      " |  min_epochs\n",
      " |  \n",
      " |  min_steps\n",
      " |  \n",
      " |  model\n",
      " |      The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\n",
      " |      \n",
      " |      To access the pure LightningModule, use\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.lightning_module` instead.\n",
      " |  \n",
      " |  node_rank\n",
      " |  \n",
      " |  num_devices\n",
      " |      Number of devices the trainer uses per node.\n",
      " |  \n",
      " |  num_nodes\n",
      " |  \n",
      " |  num_predict_batches\n",
      " |      The number of prediction batches that will be used during ``trainer.predict()``.\n",
      " |  \n",
      " |  num_sanity_val_batches\n",
      " |      The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.\n",
      " |  \n",
      " |  num_test_batches\n",
      " |      The number of test batches that will be used during ``trainer.test()``.\n",
      " |  \n",
      " |  num_training_batches\n",
      " |      The number of training batches that will be used during ``trainer.fit()``.\n",
      " |  \n",
      " |  num_val_batches\n",
      " |      The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.\n",
      " |  \n",
      " |  precision\n",
      " |  \n",
      " |  precision_plugin\n",
      " |  \n",
      " |  predict_dataloaders\n",
      " |      The prediction dataloader(s) used during ``trainer.predict()``.\n",
      " |  \n",
      " |  progress_bar_callback\n",
      " |      An instance of :class:`~pytorch_lightning.callbacks.progress.progress_bar.ProgressBar` found in the\n",
      " |      Trainer.callbacks list, or ``None`` if one doesn't exist.\n",
      " |  \n",
      " |  progress_bar_metrics\n",
      " |      The metrics sent to the progress bar.\n",
      " |      \n",
      " |      This includes metrics logged via :meth:`~pytorch_lightning.core.LightningModule.log` with the\n",
      " |      :paramref:`~pytorch_lightning.core.LightningModule.log.prog_bar` argument set.\n",
      " |  \n",
      " |  received_sigterm\n",
      " |      Whether a ``signal.SIGTERM`` signal was received.\n",
      " |      \n",
      " |      For example, this can be checked to exit gracefully.\n",
      " |  \n",
      " |  scaler\n",
      " |  \n",
      " |  strategy\n",
      " |  \n",
      " |  test_dataloaders\n",
      " |      The test dataloader(s) used during ``trainer.test()``.\n",
      " |  \n",
      " |  train_dataloader\n",
      " |      The training dataloader(s) used during ``trainer.fit()``.\n",
      " |  \n",
      " |  val_dataloaders\n",
      " |      The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.\n",
      " |  \n",
      " |  world_size\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ckpt_path\n",
      " |      Set to the path/URL of a checkpoint loaded via :meth:`~pytorch_lightning.trainer.trainer.Trainer.fit`,\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.validate`,\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.test`, or\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.predict`.\n",
      " |      \n",
      " |      ``None`` otherwise.\n",
      " |  \n",
      " |  logger\n",
      " |      The first :class:`~pytorch_lightning.loggers.logger.Logger` being used.\n",
      " |  \n",
      " |  loggers\n",
      " |      The list of :class:`~pytorch_lightning.loggers.logger.Logger` used.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          for logger in trainer.loggers:\n",
      " |              logger.log_metrics({\"foo\": 1.0})\n",
      " |  \n",
      " |  optimizers\n",
      " |  \n",
      " |  predicting\n",
      " |  \n",
      " |  sanity_checking\n",
      " |      Whether sanity checking is running.\n",
      " |      \n",
      " |      Useful to disable some hooks, logging or callbacks during the sanity checking.\n",
      " |  \n",
      " |  testing\n",
      " |  \n",
      " |  training\n",
      " |  \n",
      " |  validating\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pl.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fc4f71c-5593-464f-b6c9-69b9fb36e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/forever/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | layer_1 | Linear | 100 K  | train\n",
      "1 | layer_2 | Linear | 33.0 K | train\n",
      "2 | layer_3 | Linear | 2.6 K  | train\n",
      "-------------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|████████████████████████████████████████████████████████| 860/860 [00:09<00:00, 89.22it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|████████████████████████████████████████████████████████| 860/860 [00:09<00:00, 89.10it/s, v_num=8]\n",
      "Model saved to mnist_model_64.pt\n"
     ]
    }
   ],
   "source": [
    "model = LitMNIST()\n",
    "dm = MNISTDataModule(batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs, \n",
    "    accelerator='gpu' if torch.cuda.is_available() and device == 'cuda' else 'cpu',\n",
    "    precision=precision_model,\n",
    "    detect_anomaly=True,\n",
    "    accumulate_grad_batches=batch_size\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)\n",
    "\n",
    "torch.save(model.state_dict(), model_name)\n",
    "print(f\"Model saved to {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f07f4725-834f-47b2-9ec1-0c72401a2dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitMNIST(\n",
       "  (layer_1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (layer_2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (layer_3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LitMNIST()\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a02de2-f81f-4f25-b70a-8bc896276c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision bf16-mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d775279e-a6a8-45d3-b542-64c842effb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9619\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       980\n",
      "           1       0.98      0.98      0.98      1135\n",
      "           2       0.97      0.96      0.96      1032\n",
      "           3       0.96      0.96      0.96      1010\n",
      "           4       0.95      0.97      0.96       982\n",
      "           5       0.96      0.95      0.95       892\n",
      "           6       0.95      0.97      0.96       958\n",
      "           7       0.96      0.96      0.96      1028\n",
      "           8       0.95      0.95      0.95       974\n",
      "           9       0.96      0.94      0.95      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 963    0    1    2    1    3    7    2    1    0]\n",
      " [   0 1117    4    0    1    1    5    2    5    0]\n",
      " [   4    2  991    3    5    1    6    6   12    2]\n",
      " [   0    0    7  966    2   13    0    8   10    4]\n",
      " [   1    0    7    0  949    1    7    1    2   14]\n",
      " [   4    1    1   12    4  843   12    2    9    4]\n",
      " [   7    3    1    0    5    8  931    0    3    0]\n",
      " [   0    5   12    3    4    1    0  989    1   13]\n",
      " [   4    1    1   11    6    5   11    8  921    6]\n",
      " [   6    6    1   11   19    4    1    7    5  949]]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "test_dataset = MNIST(root=\".\", train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        outputs = model(x)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690d901-0bb8-4a36-94eb-6295f86656e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7cd133c-6cde-4365-bac5-a035a34856c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       980\n",
      "           1       0.98      0.99      0.98      1135\n",
      "           2       0.96      0.96      0.96      1032\n",
      "           3       0.95      0.96      0.95      1010\n",
      "           4       0.95      0.97      0.96       982\n",
      "           5       0.95      0.95      0.95       892\n",
      "           6       0.97      0.95      0.96       958\n",
      "           7       0.96      0.96      0.96      1028\n",
      "           8       0.96      0.94      0.95       974\n",
      "           9       0.96      0.94      0.95      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 960    0    2    1    0    6    5    3    1    2]\n",
      " [   0 1122    2    1    0    1    3    2    4    0]\n",
      " [   4    3  990    6    4    2    6    9    7    1]\n",
      " [   0    0    9  970    0    9    0   10    9    3]\n",
      " [   1    0    8    0  949    0    2    3    2   17]\n",
      " [   5    1    0   11    2  848    8    2   10    5]\n",
      " [   9    3    2    1   11   16  910    0    6    0]\n",
      " [   0   10   12    6    4    1    0  986    0    9]\n",
      " [   3    4    3   19    7    7    7    9  912    3]\n",
      " [   6    7    2   10   20    3    1    5    2  953]]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "test_dataset = MNIST(root=\".\", train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        outputs = model(x)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d04c85e-8c6a-49cd-ab87-125c11b81fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitMNIST()\n",
    "model.load_state_dict(torch.load('mnist_model.pt'))\n",
    "model.eval()\n",
    "quantizer = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "torch.save(quantizer.state_dict(), 'quant.pt')\n",
    "# quantizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "266df9e9-bd43-4090-a706-d16127d42f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9617\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       980\n",
      "           1       0.98      0.98      0.98      1135\n",
      "           2       0.97      0.96      0.96      1032\n",
      "           3       0.96      0.96      0.96      1010\n",
      "           4       0.95      0.97      0.96       982\n",
      "           5       0.96      0.95      0.95       892\n",
      "           6       0.95      0.97      0.96       958\n",
      "           7       0.97      0.96      0.96      1028\n",
      "           8       0.95      0.95      0.95       974\n",
      "           9       0.96      0.94      0.95      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 963    0    1    2    1    3    7    2    1    0]\n",
      " [   0 1117    4    0    1    1    5    2    5    0]\n",
      " [   4    2  991    3    5    1    6    6   12    2]\n",
      " [   0    0    7  966    2   13    0    8   10    4]\n",
      " [   1    0    7    0  949    1    7    1    2   14]\n",
      " [   4    1    1   12    4  843   12    2    9    4]\n",
      " [   7    3    1    0    5    8  931    0    3    0]\n",
      " [   0    5   12    3    4    1    0  989    1   13]\n",
      " [   4    1    1   11    6    5   11    7  922    6]\n",
      " [   6    7    1   11   21    4    1    7    5  946]]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "test_dataset = MNIST(root=\".\", train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        # x = x.to(device)\n",
    "        outputs = quantizer(x)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
