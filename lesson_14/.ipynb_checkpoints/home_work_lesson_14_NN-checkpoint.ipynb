{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2d2851-42e4-46c6-adca-831166281c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5eb0a1a-2736-4138-8ab5-ce318b72ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51cf713e-3a69-49fe-8444-ca3a70d75dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df - df.min()) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958e95eb-3e25-4249-93d8-432d8f4883d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature  importance\n",
      "0      V14    0.185077\n",
      "1       V4    0.120695\n",
      "2      V10    0.116924\n",
      "3      V12    0.101880\n",
      "4      V17    0.089537\n",
      "5       V3    0.064627\n",
      "6      V11    0.046515\n",
      "7      V16    0.042677\n",
      "8       V2    0.036178\n",
      "9       V9    0.026259\n",
      "10     V21    0.022648\n",
      "11      V7    0.013823\n",
      "12     V18    0.011518\n",
      "13     V19    0.011364\n",
      "14  Amount    0.009923\n",
      "15      V6    0.008869\n",
      "16      V5    0.008617\n",
      "17     V27    0.008098\n",
      "18     V13    0.007603\n",
      "19     V28    0.007559\n",
      "20     V26    0.007484\n",
      "21     V22    0.006798\n",
      "22      V8    0.006705\n",
      "23     V15    0.006655\n",
      "24      V1    0.006035\n",
      "25     V20    0.005969\n",
      "26     V25    0.005746\n",
      "27     V23    0.005162\n",
      "28    Time    0.004622\n",
      "29     V24    0.004432\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': importances\n",
    "})\n",
    "feat_imp = feat_imp.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(feat_imp)\n",
    "\n",
    "# plt.figure(figsize=(8, 10))\n",
    "# plt.barh(feat_imp['feature'], feat_imp['importance'])\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.title('Feature Importances')\n",
    "# plt.xlabel('Importance')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f86620-7b5d-4339-8da3-ae5a316454b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V14', 'V4', 'V10', 'V12', 'V17', 'V3']\n",
      "['V14', 'V4', 'V10', 'V12', 'V17', 'V3', 'Class']\n"
     ]
    }
   ],
   "source": [
    "features = feat_imp['feature'][:6].to_list()\n",
    "features_with_class =  features + ['Class']\n",
    "print(features)\n",
    "print(features_with_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ab0fa7-38a0-4da5-8a08-90e0ed93ee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebeaf14-f0f0-4acb-820f-ef49771e5538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0.0    284315\n",
       "1.0       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d261719-5e08-4c31-ba69-3c41195c49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features_with_class].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d4c177-289e-4214-8be7-e3128009ee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V14</th>\n",
       "      <th>V4</th>\n",
       "      <th>V10</th>\n",
       "      <th>V12</th>\n",
       "      <th>V17</th>\n",
       "      <th>V3</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.646053</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>0.508722</td>\n",
       "      <td>0.704193</td>\n",
       "      <td>0.731130</td>\n",
       "      <td>0.837414</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.032231</td>\n",
       "      <td>0.062764</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.037660</td>\n",
       "      <td>0.024678</td>\n",
       "      <td>0.026275</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.631744</td>\n",
       "      <td>0.214311</td>\n",
       "      <td>0.497644</td>\n",
       "      <td>0.688907</td>\n",
       "      <td>0.717074</td>\n",
       "      <td>0.821985</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.647755</td>\n",
       "      <td>0.251050</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>0.709471</td>\n",
       "      <td>0.729221</td>\n",
       "      <td>0.840530</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.662635</td>\n",
       "      <td>0.284882</td>\n",
       "      <td>0.518113</td>\n",
       "      <td>0.727494</td>\n",
       "      <td>0.742743</td>\n",
       "      <td>0.855213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 V14             V4            V10            V12  \\\n",
       "count  284807.000000  284807.000000  284807.000000  284807.000000   \n",
       "mean        0.646053       0.251930       0.508722       0.704193   \n",
       "std         0.032231       0.062764       0.022528       0.037660   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.631744       0.214311       0.497644       0.688907   \n",
       "50%         0.647755       0.251050       0.506800       0.709471   \n",
       "75%         0.662635       0.284882       0.518113       0.727494   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                 V17             V3          Class  \n",
       "count  284807.000000  284807.000000  284807.000000  \n",
       "mean        0.731130       0.837414       0.001727  \n",
       "std         0.024678       0.026275       0.041527  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.717074       0.821985       0.000000  \n",
       "50%         0.729221       0.840530       0.000000  \n",
       "75%         0.742743       0.855213       0.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5470d35-7a97-4d83-8b7f-2d2fe50aeec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61c9401a-bd8e-4395-a51a-617979c9720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# wtf ?\n",
    "batch_size = 1\n",
    "precision_model = '32'\n",
    "\n",
    "data_precision_torch = torch.float16\n",
    "match precision_model:\n",
    "    case 'bf16-mixed': \n",
    "        data_precision_torch = torch.float16\n",
    "    case '16': \n",
    "        data_precision_torch = torch.float16\n",
    "    case '32-mixed': \n",
    "        data_precision_torch = torch.float32\n",
    "    case '32': \n",
    "        data_precision_torch = torch.float32\n",
    "    case '64': \n",
    "        data_precision_torch = torch.float64\n",
    "    \n",
    "num_workers = 7\n",
    "epochs = 10\n",
    "# name_batch_precision_epochs\n",
    "model_name = f'titanic_{str(batch_size)}_{str(precision_model)}_{str(epochs)}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba25e0ae-8109-4e26-aaec-daf528dc121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   V14     284807 non-null  float64\n",
      " 1   V4      284807 non-null  float64\n",
      " 2   V10     284807 non-null  float64\n",
      " 3   V12     284807 non-null  float64\n",
      " 4   V17     284807 non-null  float64\n",
      " 5   V3      284807 non-null  float64\n",
      " 6   Class   284807 non-null  float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 15.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6504f687-3074-4e37-96c1-47c58f30e0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17213bc8-6a14-45bf-bbca-c058a2fe0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsFraud(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(6, 128)\n",
    "        self.layer_2 = nn.Linear(128, 256)\n",
    "        self.layer_3 = nn.Linear(256, 512)\n",
    "        self.layer_4 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.layer_1(x))\n",
    "        x = F.leaky_relu(self.layer_2(x))\n",
    "        x = F.leaky_relu(self.layer_3(x))\n",
    "        return self.layer_4(x).squeeze(1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds.float() == y).float().mean()\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbea3f66-e1bf-4205-8b67-38bb8386d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsFraudDataset(Dataset):\n",
    "    def __init__(self, data, targets, data_precision=torch.float64):\n",
    "        self.data = torch.tensor(data.values, dtype=data_precision)\n",
    "        self.targets = torch.tensor(targets.values, dtype=data_precision)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcec01b-b13c-4417-b4f8-422d6536c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsFraudDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch_size=64, \n",
    "                 num_workers=8, \n",
    "                 n_splits=5, \n",
    "                 fold_idx=0,\n",
    "                 shuffle_train=True,\n",
    "                 data_precision=torch.float64):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.n_splits = n_splits\n",
    "        self.fold_idx = fold_idx\n",
    "        self.shuffle_train = shuffle_train\n",
    "        self.data_precision = data_precision\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # TODO clean up me pls\n",
    "        # pd.read_csv(self.data_path)\n",
    "        # pd.read_csv(self.test_path)\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # setup train data\n",
    "        df = self.dataset\n",
    "\n",
    "        # setup target\n",
    "        X = df.drop(columns=[\"Class\"])\n",
    "        y = df[\"Class\"]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                            y,\n",
    "                                                            test_size=0.3,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=42)\n",
    "\n",
    "        # FIXME crossvalidation K-Fold ?\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        splits = list(kf.split(X_train))\n",
    "        train_idx, val_idx = splits[self.fold_idx]      \n",
    "        self.train_dataset = IsFraudDataset(X_train.iloc[train_idx], \n",
    "                                            y_train.iloc[train_idx], \n",
    "                                            data_precision=self.data_precision)\n",
    "        self.val_dataset = IsFraudDataset(X_train.iloc[val_idx], \n",
    "                                          y_train.iloc[val_idx], \n",
    "                                          data_precision=self.data_precision)\n",
    "        \n",
    "        # setup test data\n",
    "        self.test_dataset = torch.tensor(X_test.values, dtype=self.data_precision)\n",
    "        # self.test_dataset = IsFraudDataset(X_test, \n",
    "        #                                    y_test, \n",
    "        #                                    data_precision=self.data_precision) \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          num_workers=self.num_workers, \n",
    "                          persistent_workers=True,\n",
    "                          shuffle=self.shuffle_train)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          persistent_workers=True,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          persistent_workers=True,\n",
    "                          num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "567a8691-6594-4c7a-bb47-79d7bc419163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91a5b0c6-db94-4dc6-a5be-a80a26bec6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type   | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | layer_1 | Linear | 896    | train\n",
      "1 | layer_2 | Linear | 33.0 K | train\n",
      "2 | layer_3 | Linear | 131 K  | train\n",
      "3 | layer_4 | Linear | 513    | train\n",
      "-------------------------------------------\n",
      "166 K     Trainable params\n",
      "0         Non-trainable params\n",
      "166 K     Total params\n",
      "0.664     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                 | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'IsFraudDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'IsFraudDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'IsFraudDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'IsFraudDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'IsFraudDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'IsFraudDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'IsFraudDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 20131, 20132, 20134, 20136, 20137, 20138, 20139) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._data_queue.get(timeout=timeout)\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/multiprocessing/queues.py:114\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     13\u001b[39m dm = IsFraudDataModule(\n\u001b[32m     14\u001b[39m     dataset=df,\n\u001b[32m     15\u001b[39m     batch_size=batch_size, \n\u001b[32m     16\u001b[39m     num_workers=num_workers,\n\u001b[32m     17\u001b[39m     data_precision=data_precision_torch\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m trainer = pl.Trainer(\n\u001b[32m     21\u001b[39m     logger=logger,\n\u001b[32m     22\u001b[39m     callbacks=[checkpoint_callback],\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     accumulate_grad_batches=batch_size\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m trainer.fit(model, datamodule=dm)\n\u001b[32m     31\u001b[39m torch.save(model.state_dict(), model_name)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    538\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m call._call_and_handle_interrupt(\n\u001b[32m    540\u001b[39m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[32m    541\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(*args, **kwargs)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    569\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    570\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    571\u001b[39m     ckpt_path,\n\u001b[32m    572\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    573\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    574\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m \u001b[38;5;28mself\u001b[39m._run(model, ckpt_path=ckpt_path)\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    977\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m results = \u001b[38;5;28mself\u001b[39m._run_stage()\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    987\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1024\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1025\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1026\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1050\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m val_loop.run()\n\u001b[32m   1055\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1057\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:137\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     batch, batch_idx, dataloader_idx = \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx != dataloader_idx:\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m._store_dataloader_outputs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py:134\u001b[39m, in \u001b[36m_PrefetchDataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batches\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     batch = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__next__\u001b[39m()\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py:61\u001b[39m, in \u001b[36m_DataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m._start_profiler()\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[39m, in \u001b[36mCombinedLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _ITERATOR_RETURN:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     out = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator)\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator, _Sequential):\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py:142\u001b[39m, in \u001b[36m_Sequential.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     out = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterators[\u001b[32m0\u001b[39m])\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._use_next_iterator()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28mself\u001b[39m._next_data()\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28mself\u001b[39m._get_data()\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1412\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m         success, data = \u001b[38;5;28mself\u001b[39m._try_get_data()\n\u001b[32m   1413\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1414\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1256\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1255\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1257\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1258\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 20131, 20132, 20134, 20136, 20137, 20138, 20139) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "model = IsFraud()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    # ?\n",
    "    monitor=\"val_acc\",         \n",
    "    dirpath=\"checkpoints/\",     \n",
    "    filename=\"best-checkpoint\",  \n",
    "    save_top_k=3,              \n",
    "    mode=\"min\"                    \n",
    ")\n",
    "\n",
    "dm = IsFraudDataModule(\n",
    "    dataset=df,\n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers,\n",
    "    data_precision=data_precision_torch\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=epochs, \n",
    "    accelerator='gpu' if torch.cuda.is_available() and device == 'cuda' else 'cpu',\n",
    "    precision=precision_model,\n",
    "    detect_anomaly=False,\n",
    "    accumulate_grad_batches=batch_size\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)\n",
    "\n",
    "torch.save(model.state_dict(), model_name)\n",
    "print(f\"Model saved to {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8aedb5-4687-49f7-97a8-8c7a925eb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsFraud()\n",
    "model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de93e39-9cde-42bd-b4a5-279747e26613",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsTitanicPassengerDead()\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e5059-2069-4bce-aab0-98652fef70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df = df[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\n",
    "df[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].median())\n",
    "\n",
    "# setup target\n",
    "X = df.drop(columns=[\"Survived\"])\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "# crossvalidation K-Fold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splits = list(kf.split(X))\n",
    "train_idx, val_idx = splits[0]\n",
    "train_dataset = TitanicDataset(X.iloc[train_idx], y.iloc[train_idx], data_precision=data_precision_torch)\n",
    "\n",
    "test_df = pd.read_csv(test_path)\n",
    "test_df_surv = pd.read_csv('titanic/gender_submission.csv')\n",
    "test_df = test_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\n",
    "test_df[\"Sex\"] = test_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "test_df[\"Age\"] = test_df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "test_df[\"Fare\"] = test_df[\"Fare\"].fillna(df[\"Fare\"].median())\n",
    "test_dataset = torch.tensor(test_df.values, dtype=data_precision_torch)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=1, \n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f3632-f276-42a8-a225-f06b5ea54a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple test\n",
    "# 0 -> [3, 0, 22, 1, 0, 7.25]\n",
    "# 1 -> [1, 1, 38, 1, 0, 71.2833]\n",
    "with torch.no_grad():\n",
    "    out = model(torch.tensor([[3, 0, 22, 1, 0, 7.25]], device=device))\n",
    "    preds = 0 if out < 0 else 1 #torch.argmax(out, dim=0)\n",
    "    print(out)\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d96658-9f1a-48ec-8779-7720ca22d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make metrics\n",
    "y_true = []\n",
    "y_pred = []\n",
    "idx = 0\n",
    "theshold = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in test_loader:\n",
    "        x = x.to(device)\n",
    "        x = x.type(torch.float32)\n",
    "        outputs = model(x)\n",
    "        preds = 0 if outputs < theshold else 1\n",
    "        y_true.append(test_df_surv['Survived'][idx])\n",
    "        y_pred.append(preds)\n",
    "        idx = idx + 1\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235bce6-bd9a-4007-aa97-0c0907dc01d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad936c-f55c-423a-b195-7aecb7d9d357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2fb18-5259-48ee-9eec-e32186332f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
